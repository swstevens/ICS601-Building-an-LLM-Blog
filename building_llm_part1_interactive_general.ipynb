# Building an LLM, Part 1: From First Words to Fluency

**Author:** Seminar Scribe  
**Date:** November 10, 2025  
**Course:** Deploying AI at Scale

---

## Why This Matters

The question of whether machines can process human language traces back to Alan Turing's famous 1950 paper "Computing Machinery and Intelligence." Turing proposed what we now call the Turing Test: a practical test for machine intelligence based on conversational ability.

Can a machine chat with you so naturally that you can't tell it's not human? If so, Turing argued, that machine possesses a form of intelligence.

Today's Large Language Models (LLMs) like ChatGPT and Claude represent the culmination of decades of research. This journey began with mechanical computation devices and punch card programming, progressed through statistical methods, and arrived at sophisticated neural networks. Understanding this technological progression reveals not just the engineering behind modern AI systems, but the fundamental principles of pattern recognition and statistical learning that enable machines to process language.

The evolution demonstrates how increasingly sophisticated mathematical models can approximate human-like language understanding at scale.

---

## Abstract

Large Language Models don't learn language the way you might expect. Rather than being programmed with grammar rules and vocabulary lists, they learn by observing patterns across massive text collections called a **corpus** (think of a corpus as a giant library of text).

This post traces the evolution from simple **bigram** models to modern **transformers** powered by **self-attention** mechanisms. We'll explore the three-stage training **pipeline**: **pre-training** on web data, **supervised fine-tuning (SFT)** for specific tasks, and **reinforcement learning (RL)** for alignment.

**What you'll learn:**
- How raw internet text becomes training data
- How **tokenization** breaks language into digestible pieces (like breaking sentences into syllables)
- How **neural networks** learn probability patterns over billions of possible word sequences

---

## Historical Foundations: From Turing to Transformers

Before examining modern language models, let's look at where these ideas came from. The conceptual foundation for machine language processing originates with Alan Turing's 1950 proposition regarding machine intelligence, formalized in his question: "Can machines think?"

Turing proposed a practical test. A machine demonstrating conversational ability indistinguishable from human discourse would possess a form of intelligence.

### Punch Cards: The Original Discrete Encoding

The earliest computational systems processed information through punch cards. These physical cards encoded data as patterns of holes. Each position on a punch card represented a binary state: hole or no hole.

Think of it like a player piano roll, where holes in specific positions create specific notes.

This is analogous to how modern tokenization maps text to discrete numerical indices. This historical parallel illuminates a fundamental principle: effective computation requires converting continuous human concepts (words, meanings) into discrete, machine-processable representations. In other words, computers need to turn smooth, flowing language into distinct, countable pieces.

### Markov Chains: Probabilistic Foundations

The mathematical framework underlying early language models derives from Andrey Markov's work on random processes in the early 20th century. A **Markov chain** represents a system where future states depend only on the present state, not the full history.

**Weather analogy:** Like predicting tomorrow's weather based only on today's weather, not considering last week's conditions.

Applied to language, this becomes: the probability of the next word depends only on the current word (first-order Markov chain) or the preceding few words (higher-order Markov chain). **Bigram** and **trigram** models implement precisely this Markovian assumption. This represents a simplification that's simple enough to calculate but nevertheless captures significant linguistic structure.

### Connecting Past to Present

These historical elements converge in contemporary language models, which represent sophisticated extensions of these foundational concepts:
- Turing's conceptual framework (the test for intelligence)
- Punch card encoding principles (discrete representation)
- Markov's probabilistic methods (predicting from patterns)

---

## From Flashcards to Conversation: The LLM Learning Journey

Think of learning an LLM like learning a new language. You start with basic vocabulary, learn grammar through examples, practice conversation, and eventually develop fluency. LLMs follow a remarkably similar path.

### Stage 0: Gathering Your Study Materials

Before a language learner can begin studying, they need learning materials. For LLMs, this material comes from **Common Crawl**, an enormous, free repository of web pages. Think of it as essentially a snapshot of the public internet.

Imagine walking into the world's largest library where books are written in every style imaginable: news articles, forum discussions, poetry, technical manuals, and yes, even recipe blogs.

**The problem:** Not all content is useful for learning.

Imagine trying to learn French from a pile of documents that includes restaurant menus, poorly written spam, and corrupted files. You'd want to filter for high-quality content first. This is where the **data processing pipeline** comes in.

### Filtering for Quality

Using **quality heuristics** (in other words, rules of thumb for identifying valuable content), researchers filter this massive **corpus** through **data cleaning** processes. They look for signals like:
- Proper grammar
- Coherent sentence structure
- Educational value
- Substantive content

The result is a curated collection like **FineWeb Corpus**, a cleaned subset specifically designed for training language models. This is your refined textbook collection, ready for serious study.

---

## Critical Consideration: Data Quality and Bias

Beyond volume and basic quality metrics, the **data processing pipeline** must address a more serious challenge: bias propagation from training data into model behavior. Put simply, models learn and copy the biases present in their training data. This represents one of the most significant limitations of statistical learning approaches to language modeling.

### How Bias Gets Baked In

Training data reflects societal biases, stereotypes, and historical inequities present in the source material. When models learn statistical patterns from such data, they necessarily encode these biases within their learned representations.

Think of it like learning a language from old books: you might pick up outdated phrases or stereotypical expressions without realizing they're problematic.

### A Famous Example: Gender Stereotypes in Word Embeddings

A well-known case emerged from early word embedding research, demonstrating how statistical learning passes on societal biases into mathematical patterns.

The Word2Vec model, trained on Google News articles, learned vector representations that encoded gender stereotypes. The model exhibited the following **word embedding** arithmetic property:

```
embedding("woman") - embedding("man") + embedding("doctor") ≈ embedding("nurse")
```

**What this means in plain language:**

The model learned a mathematical relationship where:
- Starting with "doctor"
- Subtracting the "male" component
- Adding the "female" component
- Results in "nurse"

This reflects statistical patterns in the training corpus. "Doctor" appeared more frequently with male pronouns and contexts, while "nurse" showed stronger female associations. The model learned to encode occupational gender stereotypes as mathematical patterns. This occurred not through explicit programming, but through statistical inference from biased training data patterns.

### Three Ways Bias Shows Up

These learned biases manifest in model outputs through multiple mechanisms:

**1. Completion bias**

Models predict stereotypical continuations rather than unbiased alternatives.

*Example:* Given "The doctor examined the patient and then she..." the model may assign higher **probability** (likelihood) to continuations consistent with gendered occupational assumptions, potentially suggesting nursing activities rather than medical decisions.

**2. Association bias**

Models exhibit stronger statistical associations between concepts that co-occur in stereotypical patterns within training data:
- Occupations preferentially associated with specific genders
- Attributes correlated with racial or ethnic categories
- Capabilities linked to age or demographic characteristics

**3. Representation bias**

Certain demographic groups receive inadequate or skewed representation in training **corpora**, resulting in models that perform poorly or inappropriately when processing content related to underrepresented populations.

### What Can Be Done?

The **quality heuristics** implemented in the **data processing pipeline** must address multiple dimensions:

**Technical Quality Metrics:**
- Grammatical correctness and syntactic well-formedness
- Textual coherence and logical structure
- Information density and substantive content
- Format consistency and structural integrity

**Representational Quality Metrics:**
- Demographic representation diversity across protected categories
- Geographic and cultural coverage breadth
- Temporal currency and relevance of information
- Source diversity (avoiding overrepresentation of particular publications or perspectives)

**Bias Mitigation Strategies:**

The challenge of bias mitigation in large-scale training **corpora** requires multiple approaches:

- **Corpus-level intervention**: Statistical debiasing through targeted filtering and augmentation with counter-stereotypical examples
- **Model evaluation**: Systematic assessment of trained models for bias manifestation across protected demographic categories
- **Training pipeline integration**: The three-stage **pipeline** (pre-training, supervised fine-tuning, reinforcement learning) provides multiple intervention points

### The Reality

Modern large-scale training corpora (such as **FineWeb Corpus** derived from **Common Crawl**) contain billions of documents from diverse sources. This makes systematic bias detection and mitigation computationally and methodologically challenging.

Furthermore, bias mitigation involves value judgments regarding appropriate representation and acceptable statistical associations. These are questions without universal technical solutions. Different cultural contexts, application domains, and stakeholder groups may hold divergent views regarding what constitutes appropriate model behavior.

**Important truth:** No current methodology achieves complete bias elimination. Understanding this limitation is essential context for interpreting model capabilities, appropriate use cases, and necessary human oversight requirements in deployment contexts.

---

## Stage 1: Learning Your First Words (Pre-Training)

Just as language learners start with basic vocabulary and simple patterns, LLMs begin with elementary prediction tasks.

### Bigram Models: The Basics

The mathematical foundation for elementary language modeling derives from Markov chain theory. A **Markov chain** represents a system with randomness where future states depend solely on the present state, independent of prior history.

A **bigram model** implements a first-order Markov assumption, computing the conditional probability of word occurrence:

$$P(\text{word}_n | \text{word}_{n-1})$$

**In plain language:** Given the current word, what's the probability of each possible next word?

When presented with the sequence "I am," the model calculates probability distributions over the vocabulary:
- P("happy" | "am") = 0.30 (30% chance)
- P("hungry" | "am") = 0.15 (15% chance)
- P("learning" | "am") = 0.10 (10% chance)

These **probability** distributions represent the model's statistical inference regarding probable continuations. **Trigram** models extend this framework, considering two-word contexts:

```
P(word_n | word_n-1, word_n-2)
```

Given "I am learning," the trigram model might assign:
- P("French" | "am", "learning") = 0.40
- P("Spanish" | "am", "learning") = 0.25

### The Limitation: Context Windows

However, n-gram models exhibit fundamental limitations. Consider this sentence:

"The cat sat on the mat because it was tired"

To understand that the pronoun "it" refers to "cat" rather than "mat," you need context that may extend six or more tokens backward. This represents a resolution demanding context that bigram and trigram models simply cannot access. They have tunnel vision—they can only see the immediately preceding words.

This constraint motivated the development of **neural network**-based **language models** capable of processing extended contextual windows.

### What is Pre-Training?

**Pre-training** constitutes the foundational phase wherein models process extensive text collections (**corpora**) comprising billions of tokens, learning statistical patterns through iterative **neural network training**.

Think of it like immersion language learning: reading millions of books to absorb patterns naturally.

Rather than memorizing specific sequences, these systems develop internal representations encoding:
- Grammatical structures (how sentences are built)
- Semantic relationships (how words relate in meaning)
- Factual knowledge embedded within training data (information the text contains)

---

## Interlude: Breaking Language Into Pieces (Tokenization)

Before we can train a model, we face a fundamental challenge: computers work with numbers, but language is made of words. How do we bridge this gap?

The process of converting natural language into numerical representations processable by computational systems requires an intermediate transformation step: **tokenization**. This principle finds historical precedent in punch card systems, which encoded information as discrete patterns.

### The Punch Card Parallel

Each position on a punch card represented a binary state, creating a fixed vocabulary of representable symbols. Modern tokenization extends this concept through more sophisticated encoding schemes, but the core principle remains: convert continuous language into discrete, countable pieces.

### Why Not Just Use Whole Words?

Contemporary models cannot process raw text directly; neural networks require numerical input arrays. However, a naive word-level encoding approach presents significant challenges:

**Challenge 1: Vocabulary explosion**
Natural language contains millions of unique words. Creating a lookup table for every possible word would be impractically large.

**Challenge 2: Out-of-vocabulary problem**
Novel or rare words lack representations in fixed vocabularies. What happens when the model encounters "cryptocurrency" if it was trained before that word existed?

**Challenge 3: Morphological redundancy**
Related terms ("believe," "believable," "unbelievable") receive entirely distinct encodings despite shared morphological structure. The model has to learn each variation separately, missing the connection.

### The WordPiece Solution

The **WordPiece** algorithm implements subword tokenization, addressing these challenges through intelligent decomposition:

**Common words:** Encoded as single tokens
- "cat" → ["cat"]
- "the" → ["the"]

**Rare words:** Decomposed into subword components
- "unbelievable" → ["un", "##believ", "##able"]
- "ChatGPT" → ["chat", "##gp", "##t"]

**Novel words:** Representable through known components, enabling graceful handling

**Think of it like syllables:** Just as you can pronounce a new word by breaking it into syllables you know, the model can process new words by breaking them into subword units it has seen before.

### Advantages

This encoding strategy achieves several objectives:

1. **Fixed vocabulary size**: Typically 30,000-50,000 **tokens**, rather than millions of words
2. **Morphological awareness**: Shared subword components ("##able," "un-") encode morphological relationships
3. **Generalization capacity**: Novel words representable through known components

### From Tokens to Embeddings

Following tokenization, each token maps to a learned **word embedding**. This is a dense vector (typically 768 or 1024 dimensions) in abstract semantic space.

**Imagine organizing words on a giant map:** Words with similar meanings become neighbors in specific neighborhoods. "King" and "queen" live in the same royal neighborhood. "Cat" and "dog" share a pet district. "Bicycle" lives far away in the transportation zone.

These embeddings constitute **tensors** in computational terms: multi-dimensional arrays that neural networks process. But conceptually, they're coordinates on this giant meaning-map where the geometry encodes semantic relationships.

The geometric structure of embedding space exhibits semantic properties: semantically similar words occupy nearby regions. The embeddings for "king" and "queen" reside close together, while "king" and "bicycle" occupy distant positions.

### Vector Arithmetic

Remember the bias example? Here's the mathematical magic behind it:

```
embedding("woman") - embedding("man") + embedding("doctor") ≈ embedding("nurse")
```

This works because embeddings capture meaning as directions and distances in high-dimensional space. You can literally do math with meanings!

Each token receives assignment to a **vector of probabilities** during training. This represents the model's internal representation of contextual likelihood distributions for that token's occurrence. In other words, for each position in a sentence, the model maintains a probability distribution over all possible next tokens.

---

## The Transformer Revolution: Learning Context

Now we reach the breakthrough that made modern LLMs possible.

The architectural innovation that enabled contemporary large-scale language modeling emerged from research conducted at Google Brain and Google Research. Vaswani et al. (2017) published "Attention Is All You Need," introducing the transformer architecture that fundamentally altered natural language processing methodology.

The paper's title encapsulates its central thesis: self-attention mechanisms alone, without older recurrent or convolutional components, suffice for state-of-the-art language modeling performance.

### The Context Problem

Consider the disambiguation challenge presented by context-dependent interpretation:

**Example 1:** "The bank was steep"  
**Example 2:** "The bank was closed on Sunday"

The word "bank" has completely different meanings (riverbank vs. financial institution), and you need the full **context** of the sentence to disambiguate. How does the model know which meaning applies?

### The Self-Attention Solution

**Transformers** solve this through **self-attention** mechanisms. Think of attention as the model highlighting which words in a sentence are most relevant to understanding each other word.

**Like reading comprehension:** When you read "The bank was steep," you naturally focus on the word "steep" to understand that "bank" means riverbank, not a financial institution. You automatically ignore "closed" and "Sunday" because they don't fit this context.

When processing "bank" in "The bank was steep," the self-attention mechanism learns to pay attention to "steep" (suggesting a riverbank), not "closed" or "Sunday."

### Multi-Head Self-Attention: Multiple Perspectives

More specifically, **multi-head self-attention** is like having multiple perspectives on the same sentence simultaneously. Imagine reading a sentence while simultaneously thinking about:
- Grammar (subject-verb agreement)
- Meaning (semantic relationships)
- References (what pronouns point to)
- Context (how ideas connect)

One **attention layer** might focus on grammatical relationships (subject-verb agreement), another on semantic meaning, and another on long-range dependencies. These attention scores are computed as **probability matrices**. These are mathematical structures that encode how much each word should "attend to" every other word.

**Visualization:** Picture a spotlight system where each word can shine light on other words in the sentence. The brightness of each connection indicates relevance. When processing "it" in "The cat sat on the mat because it was tired," the spotlight from "it" shines brightest on "cat."

### Disambiguation in Action

This mechanism enables **disambiguation**: the model can determine that "it" refers to "cat" rather than "mat" by calculating which earlier word has the strongest attention connection to the pronoun.

**Similar to mental highlighting:** When you read complex text, you mentally highlight important words and phrases. Transformers do this computationally through attention weights.

The result? A model that understands language in context, not just as isolated word pairs. This is the foundation of modern **LLM** architecture.

---

## Stage 2: Learning Specific Skills (Supervised Fine-Tuning)

After pre-training, our model is like a language learner who has read extensively but never had a real conversation. They know vocabulary and grammar but need practice in specific communication styles.

This is where **Supervised Fine-Tuning (SFT)** comes in.

**Think of it as conversation practice with a tutor.** Human experts provide examples:
- Question → Helpful answer
- Instruction → Appropriate response
- Ambiguous request → Clarifying question

The model adjusts its internal parameters to perform these specific tasks well, refining the general language understanding from pre-training into practical communication skills.

**Analogy:** You might understand conversational Spanish from reading, but you need practice with a tutor to learn how to order food in a restaurant, ask for directions, or make small talk. SFT provides that specialized practice.

---

## Stage 3: Learning Preferences (Reinforcement Learning)

The final stage is **Reinforcement Learning (RL)**, where the model learns nuanced preferences. This is like learning not just to speak correctly, but to speak *appropriately* for your audience.

**Think about tone and context:** You speak differently to a child than to your boss. You adjust complexity based on your audience's expertise. You know when to be formal versus casual.

Through RL, the model learns to:
- Prioritize helpful responses over verbose ones
- Avoid harmful or biased language
- Adjust tone and complexity to match the user's needs

**How it works:** Human raters compare multiple model responses to the same prompt and indicate which are better. The model learns from this feedback, gradually adjusting its behavior to align with human preferences.

This three-stage **pipeline** (pre-training, SFT, RL) transforms statistical pattern recognition into something that feels like genuine understanding.

---

## Architectural Foundations: The Transformer Paper

The transformer architecture underlying modern LLMs derives from the seminal work by Vaswani et al. (2017), published under the title "Attention Is All You Need." This paper, produced by researchers at Google Brain and Google Research, introduced several architectural innovations that collectively enabled language modeling at unprecedented scale:

**Key Innovations:**

1. **Self-attention mechanisms** replacing recurrent processing  
   *In other words:* Instead of reading word-by-word sequentially, the model looks at all words simultaneously

2. **Positional encoding** preserving sequence order information  
   *In other words:* A way to remember where each word appears in the sentence, since we're no longer processing sequentially

3. **Multi-head attention** enabling parallel processing of multiple relationship types  
   *In other words:* Analyzing grammar, meaning, and context all at once

4. **Layer normalization and residual connections** stabilizing deep network training  
   *In other words:* Technical tricks that help very deep networks train reliably

### Why This Matters

The transformer architecture represents more than an incremental improvement. It fundamentally altered the computational tractability of language modeling.

**The key breakthrough:** By eliminating sequential processing requirements inherent in recurrent networks, transformers enable parallel computation across entire sequences. This property is critical for training models on billions of tokens.

**Analogy:** Imagine if instead of reading a book page by page, you could read all pages simultaneously and instantly understand how they connect. That's the power of parallel processing that transformers enable.

### Looking Ahead to Part 2

The subsequent article (Part 2) will examine these architectural components in greater technical detail, exploring:
- How attention heads compute relevance scores
- How positional encodings maintain sequence information in parallel processing
- How the complete transformer stack integrates these mechanisms to achieve state-of-the-art performance across diverse natural language processing tasks

---

## Reflection for a Broader Audience

The evolution from Markov chain-based bigram models to contemporary transformer architectures represents a significant advancement in computational linguistics. The progression demonstrates how increasingly sophisticated mathematical frameworks enable more nuanced language processing capabilities.

### Returning to Turing's Question

Turing's foundational question (whether machines can think) finds partial resolution in systems that, while not conscious in any human sense, demonstrate remarkable linguistic competence through statistical pattern recognition at unprecedented scale.

**The key insight:** These models don't "understand" language the way we do. They recognize and reproduce patterns with extraordinary sophistication, but there's no consciousness, no genuine comprehension behind their responses.

### Historical Connections

The historical trajectory reveals several key insights:

**First:** The encoding principle that governed punch card systems (representing complex information through discrete, machine-readable states) persists in modern tokenization schemes, albeit with substantially greater sophistication. The fundamental challenge—turning continuous concepts into discrete pieces—remains the same.

**Second:** Markov's probabilistic framework, while insufficient for capturing long-range dependencies, established the mathematical foundation upon which contemporary models build. Simple probability calculations paved the way for complex neural architectures.

**Third:** The self-attention mechanism represents not merely an architectural innovation, but a fundamental insight: contextual relationships, seemingly requiring nuanced understanding, can be reduced to simple (though computationally tractable) mathematical operations. What feels intuitive to humans can be approximated through calculation.

### What Makes LLMs Work

The three-stage training pipeline (pre-training, supervised fine-tuning, reinforcement learning) demonstrates that language competence emerges through systematic exposure to patterns at sufficient scale.

**Key realization:** Models do not learn grammar rules explicitly; rather, they discover linguistic structure through statistical inference across billions of examples. The self-attention mechanism does not "understand" context in human terms, but computes similarity matrices that approximate contextual understanding with remarkable fidelity.

**Think of it like this:** Just as Netflix recommends shows by finding patterns in millions of viewing histories (not by understanding what makes a good story), LLMs generate text by finding patterns in billions of text examples (not by understanding what words mean).

### Implications for Non-Technical Readers

Modern language models represent sophisticated pattern-matching systems that have discovered that statistical regularities in human text encode sufficient structure to enable coherent, contextually appropriate language generation.

**Important distinctions:**
- These systems are not reasoning entities possessing conscious understanding
- They don't "know" facts in the way humans know things
- They can't truly innovate or create genuinely new ideas
- They excel at recognizing, combining, and reproducing learned patterns

However, the distinction between statistical pattern recognition and genuine comprehension becomes increasingly subtle as model sophistication increases. At some point, if something behaves intelligently across enough contexts, does the underlying mechanism matter for practical purposes? This remains an open philosophical question.

### Understanding Capabilities and Limitations

Understanding the pipeline (from data collection and quality heuristics through tokenization to transformer-based training) demystifies both the capabilities and limitations of these systems.

**What they're good at:**
- Recognizing and reproducing learned patterns
- Processing context across long spans of text
- Generating fluent, grammatical text
- Adapting style and tone to context
- Making statistical inferences from training data

**What they struggle with:**
- Genuine reasoning about novel situations
- True factual accuracy (they reproduce patterns, not retrieve facts)
- Understanding causation (they see correlations)
- Tasks requiring real-world experience or common sense
- Completely eliminating training data biases

### The Bigger Picture

These systems represent powerful tools constructed on elegant mathematical principles, extending a conceptual lineage from Turing's theoretical framework through Markov's probabilistic methods to contemporary neural architectures.

The progression continues. Current research investigates:
- Multimodal learning (combining text, images, audio)
- Improved reasoning capabilities
- More efficient architectures
- Better bias mitigation strategies
- Enhanced factual accuracy

But the fundamental insight remains: language structure is learnable from statistical observation, and attention mechanisms make that learning simple enough to calculate at scale.

**Final thought:** We've created systems that can use language with remarkable fluency without understanding language in any human sense. This achievement teaches us as much about the nature of language itself—how much structure and pattern it contains—as it does about artificial intelligence.

---

## References

**Turing, A. M.** (1950). Computing machinery and intelligence. *Mind*, 59(236), 433-460.

**Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.** (2017). Attention is all you need. In *Advances in Neural Information Processing Systems* (pp. 5998-6008).

**Additional Resources:**
- Transformer architecture visualization: https://poloclub.github.io/transformer-explainer/
- HuggingFace Transformers Documentation: https://huggingface.co/docs/transformers/
- Original transformer paper (arXiv): https://arxiv.org/abs/1706.03762

---

## Glossary of Key Terms

**Bigram:** A simple model that predicts the next word based only on the current word

**Corpus:** A large collection of text used for training (plural: corpora)

**Embedding:** A numerical representation of a word as a point in high-dimensional space, where similar words are close together

**Fine-tuning:** Additional training on specific tasks after initial pre-training

**Language Model:** A system that learns to predict and generate text based on patterns in training data

**Markov Chain:** A system where future predictions depend only on the current state, not full history

**Neural Network:** A computational system inspired by biological brains, consisting of interconnected nodes that process information

**Pipeline:** The series of stages used to train an LLM (pre-training, fine-tuning, reinforcement learning)

**Pre-training:** The initial phase of training where models learn general language patterns from massive text collections

**Probability:** The likelihood of an event occurring, expressed as a number between 0 and 1 (or 0% to 100%)

**Reinforcement Learning (RL):** Training through feedback, where the model learns preferences by comparing responses

**Self-Attention:** A mechanism that allows the model to focus on relevant words when processing each word in a sentence

**Supervised Fine-Tuning (SFT):** Training on specific examples provided by human experts

**Tensor:** A multi-dimensional array of numbers that neural networks process

**Token:** A piece of text (word or subword) that the model processes as a single unit

**Tokenization:** The process of breaking text into tokens

**Transformer:** The neural network architecture that uses self-attention to process language in context

**Vector:** A list of numbers representing something (like a word) in mathematical space
